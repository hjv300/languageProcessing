{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://designguide.ku.dk/download/co-branding/ku_logo_uk_h.png\" alt=\"University logo\" width=\"300\" align=\"right\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Language processing I\n",
    "\n",
    "## Lecture 7\n",
    "\n",
    "#### Manex Agirrezabal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In this class\n",
    "\n",
    "I would like you to learn:\n",
    "\n",
    " * Text classification (sentiment analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Text classification\n",
    "\n",
    " * Assigning subject categories, topics, or genres\n",
    " * Spam detection\n",
    " * Authorship attribution\n",
    " * Age/gender identification\n",
    " * Language identification\n",
    " * Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Categorizing texts from Project Gutenberg\n",
    "\n",
    " * Is this an adventure book?\n",
    " * Is this a SciFi book?\n",
    " * Is this a Crime solving book (Agatha Christie, ...)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spam detection\n",
    "\n",
    "List of e-mails:\n",
    "\n",
    " * Email1: new offer . buy cheap viagra in our online shop\n",
    " * Email2: sweety i will arrive home late cheers i love you\n",
    " * Email3: the department meeting will be delayed until tomorrow because i will be ill\n",
    " * Email4: in our classes we will learn how to deal with drug dealers and young teenagers\n",
    " \n",
    "Given that data, we must decide if each message is Spam or not\n",
    "\n",
    "And, not only that, but also which e-mails are actually from work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Authorship attribution\n",
    "\n",
    "Imagine somebody writes a really bad thing about me in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>mmm... with what we will learn today, we could think on who can the author be!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Task definition\n",
    "\n",
    "Input:\n",
    "  * a document $d$\n",
    "  * a set of possible classes $C = \\{c_1, c_2, \\ldots, c_N\\}$\n",
    "\n",
    "Output:\n",
    "  * a predicted class $c \\in C$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The document $d$ can be of any kind.\n",
    "\n",
    " * E-mail\n",
    " * Anonymous work\n",
    " * Literary work\n",
    " * Book\n",
    " * $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How can we classify text?\n",
    "\n",
    " * Handwritten rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "if sentence contains 'viagra', 'offer', 'you won' ...\n",
    "  C=SPAM!\n",
    "else\n",
    "  C=NORMAL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How can we classify text?\n",
    "\n",
    " * Handwritten rules\n",
    "   * This systems can work really well\n",
    "   * They are really good if we don't have enough tagged data\n",
    "   * But building and maintaining them is expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How can we classify text?\n",
    "\n",
    " * Handwritten rules\n",
    " * Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How can we classify text?\n",
    "\n",
    " * Handwritten rules\n",
    " * Supervised Machine Learning, where we have the following:\n",
    "   * Input: a set of hand-labeled documents $\\{(d_1, c_{d_1}), (d_2, c_{d_2}), (d_3, c_{d_3}), \\ldots\\}$\n",
    "   * Output: a learned classifier $\\phi$\n",
    "   * Input to the $\\phi$: a new unseen document $d$\n",
    "   * Output of the classifier: a class $c \\in C$\n",
    "   \n",
    "$$\\phi{}(d) \\rightarrow c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How can we classify text?\n",
    "\n",
    " * Handwritten rules\n",
    " * Supervised Machine Learning, where, as you all know:\n",
    "   * Input: a set of hand-labeled documents $\\{(d_1, c_{d_1}), (d_2, c_{d_2}), (d_3, c_{d_3}), \\ldots\\}$\n",
    "   * Output: a learned classifier $\\phi$\n",
    "   * Input to the $\\phi$: a new unseen document $d$\n",
    "   * Output of the classifier: a class $c \\in C$\n",
    "   \n",
    "$$\\phi{}(d) \\rightarrow c$$\n",
    "   \n",
    "<center>Naive Bayes, Logistic Regression, Support Vector Machines, K-Nearest Neighbors, ...</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Today's goal\n",
    "\n",
    " * To see how can we develop a text classification system, purely based on word counts and probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Naive Bayes model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which, as its name says, it is naive, but despite its simplicity works well in some cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This model was used in a paper presented in 2002, at the EMNLP conference.\n",
    "\n",
    "EMNLP: *Empirical Methods in Natural Language Processing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><b>Thumbs up? Sentiment Classification using Machine Learning Techniques</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There will be a notebook where you will see how this model works in their data, but let's understand how learning and classification is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to predict if a text is from class $A$, $B$ or $C$. Then, we want to calculate:\n",
    "\n",
    "$$argmax( P(c_i | w_1, w_2, \\ldots, w_N))$$\n",
    "\n",
    "where $i \\in \\{A,B,C\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The probability of the class to be $c_i$, given a document composed by words $w_1, w_2, \\ldots, w_N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We seek to answer the following question:\n",
    "\n",
    "<center>Given a document, which is the class that has the highest probability?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is the highest? That's the class!\n",
    "\n",
    "$P(c_A | w_1, w_2, \\ldots, w_N)$\n",
    "\n",
    "$P(c_B | w_1, w_2, \\ldots, w_N)$\n",
    "\n",
    "$P(c_C | w_1, w_2, \\ldots, w_N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But, how can we calculate each of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(c_i | w_1, w_2, \\ldots, w_N)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bayes formula\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/18/Bayes%27_Theorem_MMB_01.jpg\" width=\"40%\" alt=\"Bayes formula\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$P(c_i | w_1, w_2, \\ldots, w_N) = \\frac{P(w_1, w_2, \\ldots, w_N | c_i) \\cdot P(c_i)}{P(w_1, w_2, \\ldots, w_N)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Two assumptions:\n",
    "\n",
    " * Bag of words assumption\n",
    " * Conditional independence assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Bag or words assumption:\n",
    "\n",
    "The position of the words does not matter\n",
    "\n",
    "<center><i>Like I wrote before , the movie is not so good.</i></center>\n",
    "\n",
    "<center>*<i>movie is Like wrote good so I not before</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Conditional independence assumption:\n",
    "\n",
    "We assume that words probabilities are independent given the class. Then:\n",
    "\n",
    "$P(w_1, w_2, \\ldots, w_N | c_i) = P(w_1 | c_i) \\cdot P(w_2 | c_i) \\cdot \\ldots \\cdot P(w_N | c_i) = \\prod_{k=1}^{N}{P(w_k | c_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Recaping what we saw before\n",
    "\n",
    "$P(c_i | w_1, w_2, \\ldots, w_N) = \\frac{P(w_1, w_2, \\ldots, w_N | c_i) \\cdot P(c_i)}{P(w_1, w_2, \\ldots, w_N)} $\n",
    "\n",
    "$P(w_1, w_2, \\ldots, w_N | c_i) = P(w_1 | c_i) \\cdot P(w_2 | c_i) \\cdot \\ldots \\cdot P(w_N | c_i) = \\prod_{k=1}^{N}{P(w_k | c_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Then, we can say this:\n",
    "\n",
    "$P(c_i | w_1, w_2, \\ldots, w_N) = \\frac{P(c_i) \\cdot \\prod_{k=1}^{N}{P(w_k | c_i)}}{P(w_1, w_2, \\ldots, w_N)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our task, we are comparing these probabilities:\n",
    "\n",
    " * $P(c_A | w_1, w_2, \\ldots, w_N)$\n",
    " * $P(c_B | w_1, w_2, \\ldots, w_N)$\n",
    " * $P(c_C | w_1, w_2, \\ldots, w_N)$\n",
    "\n",
    "and we will pick the highest one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this formula,\n",
    "\n",
    "$P(c_i | w_1, w_2, \\ldots, w_N) = \\frac{P(c_i) \\cdot \\prod_{k=1}^{N}{P(w_k | c_i)}}{P(w_1, w_2, \\ldots, w_N)} $\n",
    "\n",
    "The denominator is constant for all classes, as it doesn't depend on the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then,\n",
    "\n",
    "$argmax(P(c_i | w_1, w_2, \\ldots, w_N)) = argmax\\big(\\frac{P(c_i) \\cdot \\prod_{k=1}^{N}{P(w_k | c_i)}}{P(w_1, w_2, \\ldots, w_N)}\\big) = argmax(P(c_i) \\cdot \\prod_{k=1}^{N}{P(w_k | c_i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$argmax(P(c_i | w_1, w_2, \\ldots, w_N)) = argmax(P(c_i) \\cdot \\prod_{k=1}^{N}{P(w_k | c_i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Practical matters: How to calculate this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$P (w_k | c_i) = \\frac{count(w_k, c_i)}{\\sum_{w \\in V}{count(w, c_i)}}$\n",
    "\n",
    "$count(w_k, c_i) = $ How often does the word $w_k$ appear in documents of class $c_i$?\n",
    "\n",
    "$\\sum_{w \\in V}{count(w, c_i)} = $ How many words are there in documents of class $c_i$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P(c_i) = \\frac{no\\_documents(c_i)}{no\\_documents}$\n",
    "\n",
    "$no\\_documents(c_i) = $ Number of documents of class $c_i$\n",
    "\n",
    "$no\\_documents = $ Number of documents in the whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Then, that's it!\n",
    "\n",
    "So easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What do you think? Simple, fast, (quite) clever.\n",
    "\n",
    "<center>What else can we ask?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Practical matters\n",
    "\n",
    "<center>What happens if a word doesn't appear in the training corpus?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```P(w=Schwazenfangen | c=POS) = 0```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$argmax(P(c_i | w_1, w_2, \\ldots, w_N)) = argmax(P(c_i) \\cdot \\prod_{k=1}^{N}{P(w_k | c_i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Laplace (add-1) smoothing\n",
    "\n",
    "$P (w_k | c_i) = \\frac{count(w_k, c_i) + 1}{\\sum_{w \\in V}{(count(w, c_i) + 1)}} = \\frac{count(w_k, c_i) + 1}{|V| + \\sum_{w \\in V}{count(w, c_i)}}$\n",
    "\n",
    "$|V| = $ size of our vocabulary (including words from all classes, not just one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Laplace smoothing\n",
    "\n",
    "$P (w_k | c_i) = \\frac{count(w_k, c_i) + \\alpha}{\\alpha{} \\cdot |V| + \\sum_{w \\in V}{count(w, c_i)}}$\n",
    "\n",
    "$|V| = $ size of our vocabulary (including words from all classes, not just one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Practical matters (last one)\n",
    "\n",
    "When working with probabilities, we can have an underflow problem.\n",
    "\n",
    "In order to avoid this problem, instead of saving the probabilities, we will save the log probabilities.\n",
    "\n",
    "Then, because of logarithm properties:\n",
    "\n",
    "$log(x \\cdot y) = log(x) + log(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then,\n",
    "\n",
    "$$LogP(x) = log(P(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$argmax(P(c_i | w_1, w_2, \\ldots, w_N)) =$$\n",
    "\n",
    "\n",
    "$$argmax(P(c_i) \\cdot \\prod_{k=1}^{N}{P(w_k | c_i)}) =$$\n",
    "\n",
    "$$ argmax(log(P(c_i)) + \\sum_{k=1}^{N}{log(P(w_k | c_i))})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    " * Very fast!\n",
    " * Robust to irrelevant features\n",
    " * If the independence assumptions hold, this classifier is optimal\n",
    " * Generally, it is a good baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Does it work in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Naive Bayes in movie review data\n",
    " \n",
    " * Pang, B., Lee, L., & Vaithyanathan, S. (2002, July). [Thumbs up?: sentiment classification using machine learning techniques.](http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf) In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10 (pp. 79-86). Association for Computational Linguistics.\n",
    " \n",
    " * You can download the data from [this website](http://www.cs.cornell.edu/people/pabo/movie-review-data/) (There are different versions)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
