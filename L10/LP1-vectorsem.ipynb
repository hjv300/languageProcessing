{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://designguide.ku.dk/download/co-branding/ku_logo_uk_h.png\" alt=\"University logo\" width=\"300\" align=\"right\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Language Processing 1\n",
    "\n",
    "### Session 10 (part 2)\n",
    "\n",
    "##### Manex Agirrezabal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In the previous classes\n",
    "\n",
    "We learned about:\n",
    "\n",
    " * Lexical semantics\n",
    " * WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In this class:\n",
    "\n",
    "I would like you to learn about:\n",
    "\n",
    "  * One-hot vector representation\n",
    "  * Term-Document and Term-Term matrices\n",
    "  * TF-IDF representation\n",
    "  * Intuition of vector semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I would like to go back to a previous discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One-hot encodings or sparse representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  * Dimensionality\n",
    "    * 50,000 words in the vocabulary, then, 50,000 features\n",
    "    * can be computational resources\n",
    "  * Relation between different instances\n",
    "    * Dog, cat and table... There is the same distance between all these elements\n",
    "    * low generalization power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Check discussion in section 3 from Goldberg (2016)\n",
    "\n",
    "Goldberg, Y. (2016). A primer on neural network models for natural language processing. Journal of Artificial Intelligence Research, 57, 345-420."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Actual discussion, sparse representations vs. dense representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main idea of current word-level word embeddings is to represent words as dense representations.\n",
    "\n",
    "Advantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  * Model training will cause similar features to have similar vectors – information is shared between similar features\n",
    "    * Generalization power\n",
    "  * Computationally more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why using word embeddings?\n",
    "\n",
    "#### Modeling semantics\n",
    "\n",
    " * Until now, we modeled semantics using Wordnet (or Prolog, or something similar)\n",
    " * Search two words in Wordnet\n",
    " * Check the similarity value (according to the path in the **WORDNET GRAPH**!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * **High quality** resource, but **expensive**\n",
    " * There is a lot of manual annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we want to create a new lexical resource for a specific language\n",
    "\n",
    " * We could do it automatically\n",
    " * Take all synset-synset relations for English and...\n",
    "   * English $\\rightarrow{}$ (Our desired language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * But a lot of synonymy pairings are different across languages\n",
    " \n",
    " <center>Can you think on an example that would not work?</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('blue.v.01') turn blue\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets(\"blue\", pos='v'):\n",
    "    print (syn, syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('blue.n.07') any of numerous small butterflies of the family Lycaenidae\n",
      "Synset('blue.n.02') blue clothing\n",
      "Synset('blue.n.01') blue color or pigment; resembling the color of the clear sky in the daytime\n",
      "Synset('mold.n.05') a fungus that produces a superficial growth on various kinds of damp or decaying organic matter\n",
      "Synset('mildew.n.02') a fungus that produces a superficial (usually white) growth on organic matter\n",
      "Synset('mold.n.03') loose soil rich in organic matter\n",
      "Synset('bluing.n.01') used to whiten laundry or hair or give it a bluish tinge\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets(\"urdin\", lang=\"eus\"):\n",
    "    print (syn,syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('silver.v.02') make silver in color\n",
      "['Her worries had silvered her hair']\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets(\"urdindu\", lang=\"eus\", pos='v'):\n",
    "    print (syn,syn.definition())\n",
    "    print (syn.examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Distributional hypothesis (Harris, 1954)\n",
    "\n",
    " * Mid 20th century\n",
    " * Intuitions\n",
    "   * \"*oculist and eye-doctor occur in almost the same environments*\"\n",
    "   * \"*If A and B have almost identical environments we say that they are synonyms*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Firth (1957)\n",
    "\n",
    " * You shall know a word by the company it keeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### In Spanish:\n",
    "\n",
    "<center><i>Dime con quien andas y te diré quién eres</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example inspired by Nida (1976) found in SLP2:\n",
    "\n",
    "<center><b>pacharán</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * a bottle table of is on the\n",
    " * likes everybody\n",
    " * have dinner\n",
    " * drunk you makes\n",
    " * make blackthorn and we anis with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example inspired by Nida (1976) found in SLP2:\n",
    "\n",
    "<center><b>pacharán</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * A bottle of **pacharán** is on the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * Everybody likes **pacharán**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * We will have a **pacharán** after dinner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * **Pacharán** makes you drunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * We make **pacharán** with anis and blackthorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Conclusion:\n",
    "\n",
    " * From context words humans can guess pacharán means an alcoholic beverage made out of blackthorn and anis\n",
    "\n",
    " * Intuition for algorithm: two words are similar if they have similar word contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example KWIC\n",
    "\n",
    " * Tell me a weird word.\n",
    "\n",
    "<center><a href=\"https://corpus.byu.edu/\" target=\"_blank\"/>BYU corpus</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a word embedding?\n",
    "\n",
    "  * A representation of a word\n",
    "  * Dense representation (~300 dimensions)\n",
    "  * Why are they relevant? What properties do they have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can download pretrained word embeddings from many websites, for instance:\n",
    "\n",
    " * https://absalon.ku.dk/courses/69089/files/7371057?module_item_id=2046662 (small version of embedding file, only 50,000 words)\n",
    " * http://vectors.nlpl.eu/repository/#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are even pretrained word embeddings in the gensim package :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load vectors directly from the file\n",
    "model = KeyedVectors.load_word2vec_format('./wiki.en.vec.short50K', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we check which are the most similar words to a specific word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('houses', 0.6788979172706604),\n",
       " ('mansion', 0.6370158195495605),\n",
       " ('farmhouse', 0.6100958585739136),\n",
       " ('barn', 0.5588486194610596),\n",
       " ('representatives', 0.5545411705970764),\n",
       " ('townhouse', 0.5492976903915405),\n",
       " ('cottage', 0.5487219095230103),\n",
       " ('upstairs', 0.5245002508163452),\n",
       " ('outbuildings', 0.522941529750824),\n",
       " ('residence', 0.5194516777992249)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"house\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we check the similarity between two words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63805175"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"cat\",\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42504558"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"cat\",\"elephant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we calculate this similarity?\n",
    "\n",
    "Do it yourself:\n",
    "\n",
    "  1. Get vectors (`model.get_vector`)\n",
    "  2. Use the cosine similarity function we previously implemented (`cosine_similarity`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some nice properties:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://nlp.stanford.edu/projects/glove/images/man_woman.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Shall we see some of these operations working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paris', 2.3291807174682617),\n",
       " ('berlin', 2.1073265075683594),\n",
       " ('france', 1.9722492694854736),\n",
       " ('marseille', 1.932993769645691),\n",
       " ('toulouse', 1.9296311140060425),\n",
       " ('montpellier', 1.8973047733306885),\n",
       " ('rouen', 1.8967307806015015),\n",
       " ('avignon', 1.8911137580871582),\n",
       " ('rennes', 1.8815248012542725),\n",
       " ('ferrand', 1.8801523447036743)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model.get_vector(\"berlin\") - model.get_vector(\"germany\") + model.get_vector(\"france\")\n",
    "model.most_similar_cosmul(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 2.3895578384399414),\n",
       " ('king', 2.0380735397338867),\n",
       " ('majesty', 1.7851786613464355),\n",
       " ('monarch', 1.7262828350067139),\n",
       " ('crown', 1.6237839460372925),\n",
       " ('queens', 1.5930700302124023),\n",
       " ('whitehall', 1.5785431861877441),\n",
       " ('reign', 1.576119065284729),\n",
       " ('coronation', 1.570044994354248),\n",
       " ('royal', 1.5298391580581665)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model.get_vector(\"queen\") - model.get_vector(\"woman\") + model.get_vector(\"man\")\n",
    "model.most_similar_cosmul(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can you find more relevant patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dark', 2.387988567352295),\n",
       " ('white', 2.0791215896606445),\n",
       " ('pale', 2.0786049365997314),\n",
       " ('darker', 1.9954276084899902),\n",
       " ('darkened', 1.937967300415039),\n",
       " ('bright', 1.8600469827651978),\n",
       " ('pinkish', 1.8571182489395142),\n",
       " ('greenish', 1.8410722017288208),\n",
       " ('shades', 1.8392839431762695),\n",
       " ('dull', 1.8250279426574707)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "vec = model.get_vector(\"dark\") - model.get_vector(\"black\") + model.get_vector(\"white\")\n",
    "model.most_similar_cosmul(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also find the words that don't match in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pen'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"screen keyboard pen apple usb\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can you get more examples like these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'orange'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "model.doesnt_match(\"football basketball baseball orange rugby\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Models\n",
    "\n",
    "There are several models for estimating "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Further reading:\n",
    "\n",
    "Nice blog post: http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/\n",
    "\n",
    " - Word2vec\n",
    "   - https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
    "   - (additional) https://arxiv.org/pdf/1301.3781.pdf\n",
    " - Glove\n",
    "   - https://www.aclweb.org/anthology/D14-1162.pdf\n",
    " - FastText\n",
    "   - https://www.aclweb.org/anthology/Q17-1010.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code to play with:\n",
    "\n",
    " - Word2vec:\n",
    "   - https://rare-technologies.com/word2vec-tutorial/ (here you can also load Glove or FastText models))\n",
    "   - https://radimrehurek.com/gensim/models/word2vec.html\n",
    "   - https://code.google.com/archive/p/word2vec/\n",
    "   \n",
    " - Glove:\n",
    "   - https://github.com/stanfordnlp/GloVe\n",
    "   \n",
    " - FastText\n",
    "   - https://fasttext.cc/\n",
    "   - https://radimrehurek.com/gensim/models/fasttext.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
