{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://designguide.ku.dk/download/co-branding/ku_logo_uk_h.png\" alt=\"University logo\" width=\"300\" align=\"right\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Language Processing 1\n",
    "\n",
    "### Session 10 (part 2)\n",
    "\n",
    "##### Manex Agirrezabal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In the previous classes\n",
    "\n",
    "We learned about:\n",
    "\n",
    " * Lexical semantics\n",
    " * WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In this class:\n",
    "\n",
    "I would like you to learn about:\n",
    "\n",
    "  * One-hot vector representation\n",
    "  * Term-Document and Term-Term matrices\n",
    "  * TF-IDF representation\n",
    "  * Intuition of vector semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I would like to go back to a previous discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One-hot encodings or sparse representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  * Dimensionality\n",
    "    * 50,000 words in the vocabulary, then, 50,000 features\n",
    "    * can be computational resources\n",
    "  * Relation between different instances\n",
    "    * Dog, cat and table... There is the same distance between all these elements\n",
    "    * low generalization power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Check discussion in section 3 from Goldberg (2016)\n",
    "\n",
    "Goldberg, Y. (2016). A primer on neural network models for natural language processing. Journal of Artificial Intelligence Research, 57, 345-420."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Actual discussion, sparse representations vs. dense representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main idea of current word-level word embeddings is to represent words as dense representations.\n",
    "\n",
    "Advantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  * Model training will cause similar features to have similar vectors – information is shared between similar features\n",
    "    * Generalization power\n",
    "  * Computationally more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why using word embeddings?\n",
    "\n",
    "#### Modeling semantics\n",
    "\n",
    " * Until now, we modeled semantics using Wordnet (or Prolog, or something similar)\n",
    " * Search two words in Wordnet\n",
    " * Check the similarity value (according to the path in the **WORDNET GRAPH**!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * **High quality** resource, but **expensive**\n",
    " * There is a lot of manual annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we want to create a new lexical resource for a specific language\n",
    "\n",
    " * We could do it automatically\n",
    " * Take all synset-synset relations for English and...\n",
    "   * English $\\rightarrow{}$ (Our desired language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * But a lot of synonymy pairings are different across languages\n",
    " \n",
    " <center>Can you think on an example that would not work?</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for syn in wn.synsets(\"blue\", pos='v'):\n",
    "    print (syn, syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for syn in wn.synsets(\"urdin\", lang=\"eus\"):\n",
    "    print (syn,syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for syn in wn.synsets(\"urdindu\", lang=\"eus\", pos='v'):\n",
    "    print (syn,syn.definition())\n",
    "    print (syn.examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Distributional hypothesis (Harris, 1954)\n",
    "\n",
    " * Mid 20th century\n",
    " * Intuitions\n",
    "   * \"*oculist and eye-doctor occur in almost the same environments*\"\n",
    "   * \"*If A and B have almost identical environments we say that they are synonyms*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Firth (1957)\n",
    "\n",
    " * You shall know a word by the company it keeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### In Spanish:\n",
    "\n",
    "<center><i>Dime con quien andas y te diré quién eres</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example inspired by Nida (1976) found in SLP2:\n",
    "\n",
    "<center><b>pacharán</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * a bottle table of is on the\n",
    " * likes everybody\n",
    " * have dinner\n",
    " * drunk you makes\n",
    " * make blackthorn and we anis with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example inspired by Nida (1976) found in SLP2:\n",
    "\n",
    "<center><b>pacharán</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * A bottle of **pacharán** is on the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * Everybody likes **pacharán**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * We will have a **pacharán** after dinner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * **Pacharán** makes you drunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * We make **pacharán** with anis and blackthorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Conclusion:\n",
    "\n",
    " * From context words humans can guess pacharán means an alcoholic beverage made out of blackthorn and anis\n",
    "\n",
    " * Intuition for algorithm: two words are similar if they have similar word contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example KWIC\n",
    "\n",
    " * Tell me a weird word.\n",
    "\n",
    "<center><a href=\"https://corpus.byu.edu/\" target=\"_blank\"/>BYU corpus</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a word embedding?\n",
    "\n",
    "  * A representation of a word\n",
    "  * Dense representation (~300 dimensions)\n",
    "  * Why are they relevant? What properties do they have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can download pretrained word embeddings from many websites, for instance:\n",
    "\n",
    " * https://absalon.ku.dk/courses/69089/files/7371057?module_item_id=2046662 (small version of embedding file, only 50,000 words)\n",
    " * http://vectors.nlpl.eu/repository/#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are even pretrained word embeddings in the gensim package :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load vectors directly from the file\n",
    "model = KeyedVectors.load_word2vec_format('wiki.en.vec.short50K', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we check which are the most similar words to a specific word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"house\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we check the similarity between two words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.similarity(\"cat\",\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.similarity(\"cat\",\"elephant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we calculate this similarity?\n",
    "\n",
    "Do it yourself:\n",
    "\n",
    "  1. Get vectors (`model.get_vector`)\n",
    "  2. Use the cosine similarity function we previously implemented (`cosine_similarity`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some nice properties:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://nlp.stanford.edu/projects/glove/images/man_woman.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Shall we see some of these operations working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vec = model.get_vector(\"berlin\") - model.get_vector(\"germany\") + model.get_vector(\"france\")\n",
    "model.most_similar_cosmul(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vec = model.get_vector(\"queen\") - model.get_vector(\"woman\") + model.get_vector(\"man\")\n",
    "model.most_similar_cosmul(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can you find more relevant patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "vec = model.get_vector(\"dark\") - model.get_vector(\"black\") + model.get_vector(\"white\")\n",
    "model.most_similar_cosmul(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also find the words that don't match in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"screen keyboard pen apple usb\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can you get more examples like these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "model.doesnt_match(\"football basketball baseball orange rugby\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Models\n",
    "\n",
    "There are several models for estimating "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Further reading:\n",
    "\n",
    "Nice blog post: http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/\n",
    "\n",
    " - Word2vec\n",
    "   - https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
    "   - (additional) https://arxiv.org/pdf/1301.3781.pdf\n",
    " - Glove\n",
    "   - https://www.aclweb.org/anthology/D14-1162.pdf\n",
    " - FastText\n",
    "   - https://www.aclweb.org/anthology/Q17-1010.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code to play with:\n",
    "\n",
    " - Word2vec:\n",
    "   - https://rare-technologies.com/word2vec-tutorial/ (here you can also load Glove or FastText models))\n",
    "   - https://radimrehurek.com/gensim/models/word2vec.html\n",
    "   - https://code.google.com/archive/p/word2vec/\n",
    "   \n",
    " - Glove:\n",
    "   - https://github.com/stanfordnlp/GloVe\n",
    "   \n",
    " - FastText\n",
    "   - https://fasttext.cc/\n",
    "   - https://radimrehurek.com/gensim/models/fasttext.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
