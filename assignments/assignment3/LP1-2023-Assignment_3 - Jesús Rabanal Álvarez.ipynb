{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Processing assignment 3: Word embeddings and society\n",
    "\n",
    "In this assignment you will have to load vectorial representations of words and calculate their cosine similarity, a common distance metric to evaluate semantic similarity.\n",
    "\n",
    "On grading: There are five exercises in this assignment. You must have at least three correct exercises (and among the incorrect ones, there should be some proper attempt to solve the missing exercises). What we mean is that if you do three perfect exercises but the remaining two exercises are blank, the assignment will not be considered passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Exercise 1:\n",
    " \n",
    "In order to play with word embeddings, we need a way of storing them in our program. We need a data structure to represent all the word embeddings.\n",
    "\n",
    "The goal of this exercise is to open the file where the embeddings are saved and to put them in a variable that you can use afterwards.\n",
    "\n",
    "You can represent the data in the way that you think it fits best. The result can go from a really simple approach until a complex but useful class.\n",
    "\n",
    "Given a word, such as `\"house\"`, this data structure should return the embeddings related to that word.\n",
    "\n",
    "In the saved file, we will have a set of words, and each word will be represented as a sequence of floating point numbers, such as:\n",
    "\n",
    "`house --> 0.001 0.002 0.005 0.001 0.0012312 0.004 ...`\n",
    "\n",
    "`cow --> 0.2 0.01 0.00031 0.01 0.9 0.00031 0.0015 0.002 ...`\n",
    "\n",
    "The number of floating point numbers will always be the same.\n",
    "\n",
    "---\n",
    " \n",
    "If we want to play with word embeddings, we have to get them from somewhere. Pick English embeddings from [Absalon](https://absalon.ku.dk/files/7371057/download?download_frd=1) or the embeddings that you want from this [website](https://fasttext.cc/docs/en/crawl-vectors.html). I recommend you downloading the file from Absalon, as it contains only 50,000 words and it is easier to load (well, faster).\n",
    "\n",
    "If you get the embeddings from the Github page, you should download the embeddings in **text** format. These embeddings have been trained with raw text from Wikipedia. This may take some space in your computer, depending on the language you choose.\n",
    "\n",
    "Once you downloaded the embeddings, it's time to start programming! The files follow a specific format.\n",
    "\n",
    "##### FILE FORMAT:\n",
    "\n",
    "The first line in the file contains two numbers separated by a single space. The first number indicates the number of words in the file (`N_WORDS`) and the second number specifies the number of dimensions (`N_DIMENSIONS`) that are used to represent each of those words.\n",
    "\n",
    "After the first line, each line will contain one word at the beginning. Following the word, and separated by spaces, there will be `N_DIMENSIONS` numbers, which represent each word in the space.\n",
    "\n",
    "The words are sorted by their frequency in the wikipedia corpus, then the first words in the file will be the most frequent ones. Here you can see how the English embeddings file starts:\n",
    "\n",
    "`9999 300`\n",
    "\n",
    "`, 0.1250 -0.1079 0.0245 -0.2529 0.1057 -0.0184 0.1177 ...`\n",
    "\n",
    "`the -0.0517 0.0740 -0.0131 0.0447 -0.0343 0.0212 0.0069 ...`\n",
    "\n",
    "`. 0.0342 -0.0801 0.1162 -0.3968 -0.0147 -0.0533 0.0606 ...`\n",
    "\n",
    "`and 0.0082 -0.0899 0.0265 -0.0086 -0.0609 0.0068 0.0652 ...`\n",
    "\n",
    "`...`\n",
    "\n",
    "##### What you have to do:\n",
    "\n",
    "Write a program to read the file and store the words and their embeddings in the data structure that you think it is the best. It might be very simple, or it might be a more complex one.\n",
    "\n",
    "##### Important note: You are not allowed to use a package like gensim to open the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.8637e-02 -4.1191e-03 -2.5390e-01  3.2839e-01  1.3474e-01  2.1099e-01\n",
      "  1.5887e-01  1.3855e-01  8.4253e-03  2.1534e-01  3.0048e-01 -6.4144e-02\n",
      " -5.2371e-02 -1.1812e-01 -1.2325e-01 -4.0250e-01  3.0414e-01 -2.9539e-01\n",
      " -9.1729e-02  2.4486e-01 -4.2637e-02 -1.7132e-02 -2.8279e-01 -3.7561e-01\n",
      " -1.1360e-01  3.1833e-01  2.7959e-01 -9.7287e-02  9.9161e-03  7.5482e-02\n",
      " -9.2898e-02  3.4215e-01  1.1880e-02  7.0913e-02 -7.9244e-02 -4.6072e-01\n",
      " -1.6607e-01 -3.2089e-01 -6.7372e-02 -1.0502e-01 -9.7204e-02  1.0914e-01\n",
      " -1.5526e-01 -4.9404e-02  9.2036e-02 -2.6817e-01  1.4362e-01  2.6702e-01\n",
      " -4.4490e-02 -1.0933e-01  1.8751e-01 -1.2888e-01 -4.2224e-02  2.1953e-01\n",
      " -1.4486e-02 -1.3708e-01  6.1556e-03  2.7584e-01  2.7403e-01  2.3650e-01\n",
      " -5.9322e-02 -2.6784e-02  4.3424e-02 -2.8990e-01 -9.1051e-03 -1.1771e-01\n",
      " -2.2181e-01  2.6172e-01 -1.9857e-01 -1.1452e-03 -1.7981e-01 -2.9999e-01\n",
      " -3.1459e-02  1.4812e-01 -1.4990e-01 -8.7872e-02  2.8076e-01  3.0934e-01\n",
      " -2.3802e-01 -1.6786e-01  1.4296e-01 -3.5512e-02  2.6491e-01  2.4909e-01\n",
      " -2.6859e-01  1.3965e-01 -1.2523e-01 -8.7567e-02 -1.4256e-01 -1.5192e-01\n",
      "  4.6855e-01 -3.1569e-01  4.2575e-01 -6.4321e-02  1.4631e-01  2.9015e-01\n",
      "  1.7018e-01  1.6241e-01 -2.1218e-01 -1.3002e-01 -5.1941e-02  2.1202e-01\n",
      "  1.1423e-01  6.9989e-02 -9.0881e-02  9.1649e-03 -2.0809e-01 -9.0646e-02\n",
      " -1.1007e-01  1.9675e-01  2.0297e-01 -3.0239e-01 -7.8500e-03 -4.5531e-02\n",
      " -2.3490e-01 -3.1942e-01  2.8778e-01  1.4415e-01 -1.3988e-01 -9.8262e-02\n",
      " -2.4856e-02  2.8631e-01  7.2734e-02  3.1169e-01  1.7349e-01 -9.4652e-02\n",
      "  1.9358e-01 -7.8619e-02  4.1911e-02  2.9315e-01 -1.4378e-01 -3.2431e-03\n",
      " -1.7262e-01 -1.1287e-01 -1.4833e-01  5.2479e-02 -1.2002e-01  2.1177e-01\n",
      " -2.3899e-01  4.6967e-02  3.4401e-02  1.5859e-01  1.9077e-01 -1.3028e-01\n",
      " -1.0570e-01  5.0739e-02  7.6116e-02 -2.8666e-01  2.7083e-01  2.7517e-02\n",
      "  7.8062e-02 -1.7299e-01 -2.3644e-01  3.1155e-01 -3.5043e-01  1.0832e-02\n",
      " -2.5374e-01 -1.9897e-01  3.9576e-01  1.4564e-01 -1.6733e-01  3.5596e-01\n",
      " -1.4860e-01  1.0577e-02  7.7298e-02 -1.0801e-01 -2.5220e-02 -2.7463e-01\n",
      "  3.5758e-02  8.7457e-03  3.0075e-01 -1.5060e-01 -3.7348e-01 -1.4688e-04\n",
      "  8.0223e-02  3.9295e-02  2.4266e-02  1.2489e-01 -1.8261e-01 -1.5411e-01\n",
      "  2.5599e-01  1.3696e-01 -7.5897e-02  2.3621e-01 -1.1996e-01  5.9082e-02\n",
      "  4.5113e-03 -2.6500e-02 -1.5862e-01  1.4833e-01 -1.3717e-01 -7.5295e-02\n",
      "  3.6068e-03  3.3430e-01 -4.7609e-01 -1.0999e-01  2.5772e-03  2.0641e-01\n",
      " -3.0106e-01 -1.4034e-01  3.0545e-01  2.2107e-01 -1.7733e-01  1.4956e-01\n",
      " -1.0698e-01 -4.1806e-02 -1.5489e-01 -9.2820e-02  3.4236e-01  2.4458e-01\n",
      "  4.1244e-01 -2.2156e-01 -1.9863e-01 -1.1431e-01  4.8688e-02 -3.8552e-02\n",
      " -9.3706e-02 -1.3012e-01  1.4795e-01 -2.5128e-02 -1.8354e-01 -1.6991e-01\n",
      "  1.6199e-01  6.5366e-03 -3.8862e-01 -1.1147e-01 -2.7128e-01 -1.5628e-01\n",
      "  1.6608e-01 -7.6110e-02  2.7857e-01  3.2981e-01 -7.7059e-02 -1.7226e-01\n",
      " -6.0446e-02  1.8249e-01 -1.4718e-01  7.8722e-02 -2.3768e-02 -1.0865e-01\n",
      "  2.5488e-02  1.0844e-02  2.5315e-01  1.9115e-01 -3.5454e-01  1.4154e-01\n",
      " -7.6163e-03 -6.5071e-02 -3.6534e-01  3.2655e-03  1.2861e-01 -1.7817e-01\n",
      " -4.1918e-02 -4.2774e-02 -6.3015e-02 -8.1107e-02 -5.1664e-02 -1.0624e-01\n",
      "  3.8855e-01 -7.8108e-02 -7.1574e-03 -6.9308e-02  2.1146e-01 -3.8771e-01\n",
      " -6.8994e-02  2.4726e-02 -1.2444e-01  1.8235e-02  1.7583e-01 -2.0191e-01\n",
      " -1.3731e-01 -8.3628e-02 -1.3808e-01  1.0710e-01 -3.7167e-01 -1.6071e-01\n",
      "  2.4343e-01 -1.3551e-02  1.3299e-01 -4.4403e-02  1.7013e-02  1.5802e-01\n",
      " -2.8619e-01  1.7508e-01 -8.2188e-02  1.2236e-01  1.7264e-02  3.4080e-02\n",
      "  6.8108e-02 -1.2794e-01  1.5692e-01 -3.3882e-01 -9.9046e-02 -1.0486e-01\n",
      " -4.8754e-03  1.2953e-01 -1.7471e-01  1.2390e-01  4.6296e-02  2.7141e-01]\n"
     ]
    }
   ],
   "source": [
    "#1.- Define object to save words and their embeddings\n",
    "#2.- Write code for reading the file and save it in the defined object\n",
    "\n",
    "#1. I decided the best object to save this type of data was a dictionary i.e.: word_embeddings\n",
    "#2. Here it is the code to do this, and a short print statement to show the use of it\n",
    "\n",
    "with open('wiki.en.vec.short50K', 'r', encoding='utf-8') as f:\n",
    "    word_embeddings = {}\n",
    "    for line in f:\n",
    "        elements = line.split()\n",
    "        word = elements[0]\n",
    "        embedding = np.array([float(x) for x in elements[1:]])\n",
    "        word_embeddings[word] = embedding\n",
    "\n",
    "print(word_embeddings['she'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:\n",
    "\n",
    "A common distance metric used to measure the similarity between two words is the cosine similarity, which measures the cosine of the angle between the two vectors that represent each of the words.\n",
    "\n",
    "This similarity value is calculated by using this formula:\n",
    "\n",
    "$$\\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\|_2 \\|\\mathbf{B}\\|_2} $$\n",
    "\n",
    "<!--= \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }-->\n",
    "\n",
    "Don't be scared. The first part of the formula, $\\mathbf{A} \\cdot \\mathbf{B}$ is the dot product between vectors $\\mathbf{A}$ and $\\mathbf{B}$. And you know how to do that in Python.\n",
    "\n",
    "$\\mathbf{A} \\cdot \\mathbf{B} = \\sum\\limits_{i=1}^{n}{A_i  B_i}$\n",
    "\n",
    "In the lower part, $\\|\\mathbf{A}\\|_2 \\|\\mathbf{B}\\|_2$, you have to calculate the Euclidean norm of each vector ($\\mathbf{A}$ and $\\mathbf{B}$) and multiply their results. The Euclidean norm is calculated using this formula:\n",
    "\n",
    "$\\|\\mathbf{A}\\|_2 = \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}$\n",
    "\n",
    "The formula inside the square root is the same as the one from the dot product. Then it can be rewritten like this:\n",
    "\n",
    "$\\|\\mathbf{A}\\|_2 = \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}} = \\sqrt{\\mathbf{A} \\cdot \\mathbf{A}} $\n",
    "\n",
    "You should program the cosine similarity function by using numpy. You cannot use previously programmed cosine similarity functions, you must write your own function. This program must get two numpy arrays and it should return a number.\n",
    "\n",
    "The resulting number of this formula should be interpreted as a number that specifies the similarity between two words. The higher the number, the similarity between those two words will be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'house' and 'he': 0.2716942456042166\n",
      "Cosine similarity between 'the' and 'he': 0.4304674538970477\n"
     ]
    }
   ],
   "source": [
    "def similarity(A, B):\n",
    "    #YOUR CODE HERE\n",
    "    cosine_similarity = (np.dot(A, B)) / ((np.sqrt(np.dot(A, A))) * (np.sqrt(np.dot(B, B))))\n",
    "    return cosine_similarity\n",
    "\n",
    "A = word_embeddings['house']\n",
    "B = word_embeddings['he']\n",
    "C = word_embeddings['the']\n",
    "\n",
    "print(\"Cosine similarity between 'house' and 'he':\", similarity(A, B))\n",
    "print(\"Cosine similarity between 'the' and 'he':\", similarity(C, B))\n",
    "#It makes sense that 'she' and 'he' are more 'similar' than 'the' and 'he'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3:\n",
    "\n",
    "In the third exercise you have to squeeze your brain a bit more. Now, you have loaded the whole embedding file, and you also have a distance metric to measure the similarity between words. Let's do more complex things, then.\n",
    "\n",
    "Given a word, you have to find the 30 most similar words. Then, given one word you should get the distance to all the words in the embeddings file, and pick the nearest ones.\n",
    "\n",
    "In order to make this task easier, I attach a simple implementation of an ordered list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function should return the embeddings of a word according to your class\n",
    "def get (LIST, index):\n",
    "    return LIST[index]\n",
    "\n",
    "def get_value(el):\n",
    "    return el[1]\n",
    "\n",
    "\n",
    "\n",
    "class OrderedListTuple:\n",
    "    \n",
    "    def __init__(self, max_size):\n",
    "        self.content = []\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def find_pos (self, element):\n",
    "        index = 0\n",
    "        while (index <= len(self.content)-1) and get_value(get(self.content, index)) > get_value(element):\n",
    "            index += 1\n",
    "        return index\n",
    "\n",
    "    def insert_element (self, element):\n",
    "        pos = self.find_pos (element)\n",
    "        self.content.insert (pos, element)\n",
    "        if len(self.content) > self.max_size:\n",
    "            self.content.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is very simple. When we initialize the list, we set the number of elements that it will have at most. Then, when we add elements to the list, it will add the element in the correct position. But, if the number of elements is higher than the ones that we can keep, the object will remove the last element. Let's see how it works with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[('house', 14)]\n",
      "[('house', 14), ('home', 6)]\n",
      "[('house', 14), ('home', 6), ('brown', 3)]\n",
      "[('house', 14), ('home', 6), ('elbow', 4), ('brown', 3)]\n",
      "[('house', 14), ('home', 6), ('elbow', 4), ('brown', 3)]\n",
      "[('house', 14), ('the', 9), ('home', 6), ('elbow', 4)]\n",
      "[('and', 43), ('house', 14), ('the', 9), ('home', 6)]\n",
      "[('kitty', 44), ('and', 43), ('house', 14), ('the', 9)]\n"
     ]
    }
   ],
   "source": [
    "L = OrderedListTuple(4)\n",
    "print (L.content)\n",
    "\n",
    "L.insert_element((\"house\", 14))\n",
    "print (L.content)\n",
    "L.insert_element((\"home\", 6))\n",
    "print (L.content)\n",
    "L.insert_element((\"brown\", 3))\n",
    "print (L.content)\n",
    "L.insert_element((\"elbow\", 4))\n",
    "print (L.content)\n",
    "L.insert_element((\"high\", 1))\n",
    "print (L.content)\n",
    "L.insert_element((\"the\", 9))\n",
    "print (L.content)\n",
    "L.insert_element((\"and\", 43))\n",
    "print (L.content)\n",
    "L.insert_element((\"kitty\", 44))\n",
    "print (L.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: Why don't you create a similarity function that gets two words, and it returns a tuple? For each word in the dictionary, you can calculate the similarity to an input word, and save this in a tuple. Then, using the previous data structure, you can save only the N-best words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data structure, you should be able to get the most similar words to one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 most similar words to: house\n",
      " ['houses', 'mansion', 'farmhouse', 'barn', 'representatives', 'townhouse', 'cottage', 'upstairs', 'outbuildings', 'residence', 'room', 'schoolhouse', 'passedbody', 'bedroom', 'bungalow', 'mansions', 'parsonage', 'downstairs', 'barns', 'tavern', 'cottages', 'apartment', 'manor', 'bedrooms', 'kitchen', 'townhouses', 'hallway', 'rooms', 'lodgings', 'rectory']\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "given_word = 'house'\n",
    "\n",
    "target_vector = word_embeddings[given_word]\n",
    "\n",
    "ordered_list = OrderedListTuple(max_size=30)\n",
    "\n",
    "for word, vector in word_embeddings.items():\n",
    "    if word != given_word:\n",
    "        if target_vector.shape == vector.shape:\n",
    "            #print(target_vector, word)\n",
    "            similarity_points = similarity(target_vector, vector)\n",
    "\n",
    "            new_word = (word, similarity_points)\n",
    "\n",
    "            ordered_list.insert_element(new_word)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "similar_words = [word for word, _ in ordered_list.content]\n",
    "print(f'30 most similar words to: {given_word}\\n', similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4:\n",
    "\n",
    "The last exercise is really cool. One of the properties that researchers found in word embeddings was that we could perform algebraic operations over the vectors in order to get specific words.\n",
    "\n",
    "For example, if we perform an operation like this one:\n",
    "\n",
    "$$DICTIONARY['berlin'] - DICTIONARY['germany'] + DICTIONARY['france']$$\n",
    "\n",
    "This results in a vector. If we find the 20 closest words to that vector, we should be able to see that the word `Germany` will be near. Another nice operation was:\n",
    "\n",
    "$$DICTIONARY['queen'] - DICTIONARY['woman'] + DICTIONARY['man']$$\n",
    "\n",
    "Perform this operations with the words you want, and check if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "#berlin - germany + france"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "#queen - woman + man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5:\n",
    "\n",
    "In recent years, many researchers have shown that word embeddings obtained from large corpora reproduce biases that happen in society.\n",
    "\n",
    "In this exercise, we would like to ask you to try to show some examples in the loaded word embedding file that show some sort of bias. This bias can be either of these, or any other bias you are interested:\n",
    "\n",
    " * Gender\n",
    " * Origin\n",
    " * Sexual preference\n",
    " * Socioeconomic class\n",
    " * Academic background\n",
    " \n",
    "These examples could be based on distances between words, but any other creative methodology that you could think of will be well considered as well.\n",
    "\n",
    "For instance, what is the distance between \"maid\" and \"man\", and \"maid\" and \"woman\"?\n",
    "\n",
    "You should provide examples but also your interpretation of these results.\n",
    "\n",
    "If you want to get some inspiration, you may want to check some recent articles about the topic:\n",
    "\n",
    "  * Bender, Emily M., and Batya Friedman. \"Data statements for natural language processing: Toward mitigating system bias and enabling better science.\" Transactions of the Association for Computational Linguistics 6 (2018): 587-604. https://aclanthology.org/Q18-1041/\n",
    "  * Hovy, Dirk, and Shrimai Prabhumoye. \"Five sources of bias in natural language processing.\" Language and Linguistics Compass 15.8 (2021): e12432. https://compass.onlinelibrary.wiley.com/doi/10.1111/lnc3.12432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR INTERPRETATION HERE (press ENTER to write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "In this exercise we are going to see how to calculate word counts normalized by document frequency (TF-IDF).\n",
    "\n",
    "To this end, we will calculate the word frequencies and the IDF normalized counts using a Python package called TFIDFvectorizer from Scikit-Learn.\n",
    "\n",
    "We will work with the Gutenberg corpus from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileids = gutenberg.fileids()\n",
    "fileids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for fileid in fileids:\n",
    "    corpus.append(nltk.corpus.gutenberg.raw(fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 1:\n",
    "vec_tfidf = TfidfVectorizer()\n",
    "result_tfidf = vec_tfidf.fit_transform(corpus).toarray()\n",
    "\n",
    "#Step 2:\n",
    "id2word = {vec.vocabulary_[key]:key for key in vec.vocabulary_.keys()}\n",
    "\n",
    "#Step 3:\n",
    "sorted_ids_blake = np.argsort(result_tfidf[4]).reshape(-1)\n",
    "\n",
    "#Step 4:\n",
    "for id in sorted_ids_blake[-10:]:\n",
    "    print (id2word[id],result_tfidf[4][id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6.1:\n",
    "Explain using your own words and in one single sentence (per step) what happens in each step.\n",
    "\n",
    "###### Step 1: YOUR SENTENCE HERE\n",
    "\n",
    "###### Step 2: YOUR SENTENCE HERE\n",
    "\n",
    "###### Step 3: YOUR SENTENCE HERE\n",
    "\n",
    "###### Step 4: YOUR SENTENCE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.2:\n",
    "Can you check what are the top-10 most relevant words based on their inverse document frequency? I am asking for the 10 words with the highest inverse document frequency.\n",
    "\n",
    "If you do not know how to get the IDFs of the words, you may want to take a look at the documentation of the TFIDFvectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.3:\n",
    "\n",
    "How do the inverse document frequencies look like in this corpus? Do they seem relevant? Please state that in 1-2 sentences\n",
    "\n",
    "#### YOUR RESPONSE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
