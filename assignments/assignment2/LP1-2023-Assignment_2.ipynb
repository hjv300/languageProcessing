{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Processing 1: assignment 2\n",
    "\n",
    "**Deadline: Nov 16, 23:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment has three parts:\n",
    "\n",
    "In the first part, you will have to analyze and extend the Minimum Edit Distance algorithm to return an alignment, and not only the minimum number of operations **(20% of the grade)**.\n",
    "\n",
    "In the second part, you will train some language recognizers based on language models and you will be asked to find out which are the errors that the model is making **(20% of the grade)**.\n",
    "\n",
    "The last part will be about sentiment analysis, where you will implement the Naive Bayes model that we saw in  class **(60% of the grade)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Getting an alignment using Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we give you an implementation of Minimum Edit Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1 1 - 2.0 2.0 0.0 s\n",
      "1.0 1 2 - 3.0 1.0 3.0 d\n",
      "2.0 1 3 - 4.0 2.0 4.0 d\n",
      "3.0 1 4 - 5.0 3.0 5.0 d\n",
      "4.0 1 5 - 6.0 4.0 4.0 d\n",
      "1.0 2 1 - 1.0 3.0 3.0 i\n",
      "2.0 2 2 - 2.0 2.0 2.0 i\n",
      "3.0 2 3 - 3.0 3.0 3.0 i\n",
      "2.0 2 4 - 4.0 4.0 2.0 s\n",
      "3.0 2 5 - 5.0 3.0 5.0 d\n",
      "2.0 3 1 - 2.0 4.0 2.0 i\n",
      "3.0 3 2 - 3.0 3.0 3.0 i\n",
      "4.0 3 3 - 4.0 4.0 4.0 i\n",
      "3.0 3 4 - 3.0 5.0 5.0 i\n",
      "2.0 3 5 - 4.0 4.0 2.0 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops=['i','d','s']\n",
    "\n",
    "def MED (src_str,trg_str):\n",
    "    src_str = \"#\"+src_str\n",
    "    trg_str = \"#\"+trg_str\n",
    "    \n",
    "    ins_cost = 1\n",
    "    del_cost = 1\n",
    "    sub_cost = 2\n",
    "    \n",
    "    m = len(src_str)\n",
    "    n = len(trg_str)\n",
    "    \n",
    "    #INITIALIZE DISTANCE MATRIX WITH ZEROS\n",
    "    distance_matrix = np.zeros((n,m))\n",
    "\n",
    "    #INITIALIZE COLUMN 0 VALUES\n",
    "    distance_matrix [:,0] = np.arange(0,n,del_cost)\n",
    "    #INITIALIZE ROW 0 VALUES\n",
    "    distance_matrix [0,:] = np.arange(0,m,ins_cost)    \n",
    "    \n",
    "    for i in range(1,n): #each column\n",
    "        for j in range(1,m): #each row\n",
    "            insert = distance_matrix[i-1,j] + ins_cost\n",
    "            delete = distance_matrix[i,j-1] + del_cost\n",
    "            if src_str[j]==trg_str[i]:\n",
    "                substi = distance_matrix[i-1,j-1]\n",
    "            else:\n",
    "                substi = distance_matrix[i-1,j-1] + sub_cost\n",
    "\n",
    "            distance_matrix[i,j] = min([insert,delete,substi])\n",
    "            which_op = np.argmin([insert,delete,substi])\n",
    "            \n",
    "            print (distance_matrix[i,j],i,j, \"-\",insert,delete,substi, ops[which_op])\n",
    "            \n",
    "            \n",
    "    #If you uncomment the following command you can see\n",
    "    #the distance matrix in the same way\n",
    "    #that appears in Figure 3.27 from the SLP2 book, page 111.\n",
    "#    print (np.flip(distance_matrix.T, axis=0))\n",
    "    \n",
    "    #RETURN THE LAST ELEMENT\n",
    "    return distance_matrix[-1,-1]\n",
    "\n",
    "MED (\"PRNAP\",\"PAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1 (20 pts):\n",
    "\n",
    "You have to extend the given implementation so that it returns one alignment. Considering one minimum edit distance, there might be more than one alignment. If you return one possible alignment, the exercise will be considered correct.\n",
    "\n",
    "The alignment should be a sequence of actions that should be applied to the source string. Considering the source string \"intention\" and the target string \"execution\", the function could return\n",
    "\n",
    "`['d','s','s','-','i','s','-','-','-','-']`\n",
    "\n",
    "where `d` represents deletion, `i` represents insertion, `s` represents substitution and `-` represents no change. This result could be used to represent the two aligned strings as in Figure 3.23 in SLP2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'd', 'd', '-', 'd', 'i', 'i', 'i', 'i', '-', '-', '-', '-']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MED_alignment(src_str, trg_str):\n",
    "    src_str = \"#\" + src_str\n",
    "    trg_str = \"#\" + trg_str\n",
    "    \n",
    "    ins_cost = 1\n",
    "    del_cost = 1\n",
    "    sub_cost = 2\n",
    "    \n",
    "    m = len(src_str)\n",
    "    n = len(trg_str)\n",
    "    \n",
    "    alignment = [] #list of actions\n",
    "\n",
    "    #INITIALIZE DISTANCE MATRIX WITH ZEROS\n",
    "    distance_matrix = np.zeros((n,m))\n",
    "\n",
    "    #INITIALIZE COLUMN 0 VALUES\n",
    "    distance_matrix [:,0] = np.arange(0,n,del_cost)\n",
    "    #INITIALIZE ROW 0 VALUES\n",
    "    distance_matrix [0,:] = np.arange(0,m,ins_cost)\n",
    "    \n",
    "    bck = np.zeros((n,m),dtype=str)\n",
    "    \n",
    "    for i in range(1,n): #each column\n",
    "        for j in range(1,m): #each row\n",
    "            insert = distance_matrix[i-1,j] + ins_cost\n",
    "            delete = distance_matrix[i,j-1] + del_cost\n",
    "            if src_str[j]==trg_str[i]:\n",
    "                substi = distance_matrix[i-1,j-1]\n",
    "            else:\n",
    "                substi = distance_matrix[i-1,j-1] + sub_cost\n",
    "                \n",
    "            distance_matrix[i,j] = min([insert,delete,substi])\n",
    "            which_op = np.argmin([insert,delete,substi])\n",
    "            bck[i,j] = ops[which_op]\n",
    "            \n",
    "            #print(distance_matrix[i,j],i,j, \"-\",insert,delete,substi, ops[which_op])\n",
    "    #YOUR CODE HERE\n",
    "    row, column = n - 1, m - 1\n",
    "    while row > 0 or column > 0:\n",
    "        if row == 0:\n",
    "            alignment.append(\"d\")\n",
    "            column -= 1\n",
    "        elif column == 0:\n",
    "            alignment.append(\"i\")\n",
    "            row -= 1\n",
    "        else:\n",
    "            if bck[row, column] != 's':\n",
    "                if bck[row, column] == 'i':\n",
    "                    alignment.append(\"i\")\n",
    "                    row -= 1\n",
    "                else:\n",
    "                    alignment.append(\"d\")\n",
    "                    column -= 1\n",
    "            else:\n",
    "                if src_str[column] == trg_str[row]:\n",
    "                    alignment.append(\"-\")\n",
    "                else:\n",
    "                    alignment.append(\"s\")\n",
    "                row -= 1\n",
    "                column -= 1\n",
    "    alignment.reverse()\n",
    "    print(alignment)\n",
    "    #If you uncomment the following command you can see\n",
    "    #the distance matrix in the same way\n",
    "    #that appears in Figure 3.27 from the SLP2 book, page 111.\n",
    "    #print(np.flip(distance_matrix.T, axis=0))\n",
    "    \n",
    "    #RETURN THE LAST ELEMENT\n",
    "    return distance_matrix[-1,-1]\n",
    "\n",
    "MED_alignment (\"intention\",\"execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Language Models\n",
    "\n",
    "In the following cells of code, we show you a simple language recognizer based on written bibles, available on Github (https://github.com/christos-c/bible-corpus).\n",
    "\n",
    "Please download the whole repository (https://github.com/christos-c/bible-corpus). You can download it from the website and you should save a folder called bible-corpus.\n",
    "\n",
    "If you prefer, you can clone the repository by executing the following cell (This command will work for those that have a Unix based system and have Git installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'bible-corpus' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/christos-c/bible-corpus.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a folder called bible-corpus, which contains another folder called bibles with a number of XML files. Each of these XML files contains a bible in a specific language, encoded in XML format.\n",
    "\n",
    "The following piece of code will read some corpora. You can specify which languages you read by specifying their codes in the variable `languages`, separated by spaces. Now it loads Spanish(es), Danish(da), English(en), Basque(eu), Swedish(sv) and Finnish(fi) corpora and it saves each corpus in a dictionary called `corpus`. We can access each corpus, which is just raw text, by writing `corpus[\"es\"]` (this will get us the spanish corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................bible-corpus/bibles/Italian.xml\n",
      "..............................................bible-corpus/bibles/Spanish.xml\n",
      ".bible-corpus/bibles/Romanian.xml\n",
      "............bible-corpus/bibles/Portuguese.xml\n",
      "......bible-corpus/bibles/Danish.xml\n",
      "........................"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "languages = \"es da ro pt it\".split()\n",
    "\n",
    "\n",
    "corpus = {}\n",
    "for file in glob.glob(\"bible-corpus/bibles/*.xml\"):\n",
    "    print (\".\",end=\"\")\n",
    "    et = etree.parse(file)\n",
    "    r  = et.getroot()\n",
    "    \n",
    "    language = r.findall(\"cesHeader/profileDesc/langUsage/language\")[0].attrib['id']\n",
    "    \n",
    "    if language in languages:\n",
    "        print (file)\n",
    "        segs = r.findall(\"text/body/div/div/seg\")\n",
    "    \n",
    "        corpus[language]  = \" \".join([seg.text.strip().lower() for seg in segs if seg.text is not None])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no princípio criou deus os céus e a terra. a terra'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at how the corpus starts in, e.g. English:\n",
    "\n",
    "corpus['pt'][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, I made a simple program (`train_lm`) to train a language model given a text. Then I included a simple function `return_proba` that gets a probability distribution and a sentence, and it returns the probability of that sentence to belong to that language.\n",
    "\n",
    "If you want to know how this works, please feel free to check out [this simple notebook](https://absalon.ku.dk/files/7716003/download?download_frd=1) that shows how you can make a simple language recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_lm(data):\n",
    "    # data I/O\n",
    "    chars = list(set(data))\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "    print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "    cfreq_bigrams_corpus= nltk.ConditionalFreqDist(nltk.bigrams(data))\n",
    "    cprob_bigrams_corpus = nltk.ConditionalProbDist(cfreq_bigrams_corpus, nltk.MLEProbDist)\n",
    "    return cprob_bigrams_corpus\n",
    "\n",
    "def prod (ite):\n",
    "    if len(ite)==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return ite[0]*prod(ite[1:])\n",
    "    \n",
    "\n",
    "def return_proba (cpd, sentence):\n",
    "    return prod([cpd[bigram[0]].prob(bigram[1]) if cpd[bigram[0]].prob(bigram[1]) != 0 else 1 for bigram in nltk.bigrams(sentence)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a set of language models for the languages initially specified and it saves each distribution in a variable called `probability_distributions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "data has 3912619 characters, 56 unique.\n",
      "da\n",
      "data has 3555654 characters, 55 unique.\n",
      "ro\n",
      "data has 3952588 characters, 44 unique.\n",
      "pt\n",
      "data has 3826752 characters, 65 unique.\n",
      "it\n",
      "data has 3809385 characters, 56 unique.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probability_distributions = {}\n",
    "for language in languages:\n",
    "    print (language)\n",
    "    probability_distributions[language] = train_lm(corpus[language])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see a function that gets a sentence and a probability distribution as input, and it returns a list of tuples. Each tuple contains a language and a probability of belonging to that language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Episoden med de mange skotske fans, der lagde et voldsomt tryk på udebanefansenes indgang til Brøndby Stadion, hvor politiet måtte trække stavene, har resulteret i én anholdelse.\".lower()\n",
    "\n",
    "def which_language(sentence, p_d):\n",
    "    return [(lang, return_proba(p_d[lang],sentence)) for lang in p_d.keys()] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('es', 5.376541598888056e-210),\n",
       " ('da', 1.546902713190505e-193),\n",
       " ('ro', 7.113738359939542e-206),\n",
       " ('pt', 5.146648035275378e-205),\n",
       " ('it', 5.87111804766044e-215)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can see in this case that the model is able to detect that the text is written in Danish,\n",
    "#as it is the language that shows the highest probability.\n",
    "\n",
    "which_language(sent, probability_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1 (20 pts):\n",
    "\n",
    "Select five languages from the corpus and train a language recognizer that can distinguish between those languages. The only thing that you have to do for this is to change the variable `languages` that on top with the languages that you desire to work with. The languages must have either the same or a similar alphabet.\n",
    "\n",
    "Once that you selected the languages that you want to work with, you should look for (at least) five examples that the model fails to get the correct language. And you should try to justify why that is happening. Why does the model fail to guess those languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Sentence 1: \"Ey bro que no te coscas\" #Using english-loan words in spanish\n",
    " * Sentence 2: \"Brandon ganó 9 premios aprox. \" #Using abbreviations and numbers\n",
    " * Sentence 3: \"Pietro veut acheter une playstation.\" #Using proper from other languages\n",
    " * Sentence 4: \"illo pollica k m'enterao k la ma tadicho k nanai de juga a la play\" #Using very unformal and colloquial southern spanish syntax\n",
    " * Sentence 5: \"La dualidad onda-partícula en la mecánica cuántica desafía nuestra intuición clásica sobre la naturaleza fundamental de las partículas subatómicas.\" #Using complex scientific jargon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: [('es', 8.875523687655956e-21), ('da', 5.02968740083797e-28), ('ro', 1.0587158038726084e-23), ('pt', 2.610185114066193e-18), ('it', 1.486785821716685e-19)]\n",
      "Sentence 2: [('es', 4.161843526954122e-39), ('da', 1.3106036438756067e-39), ('ro', 1.6179960461392655e-30), ('pt', 2.6170073864870195e-35), ('it', 8.913147417462332e-26)]\n",
      "Sentence 3: [('es', 5.727638188599018e-43), ('da', 1.3721953858557431e-44), ('ro', 5.3467455764367683e-39), ('pt', 4.691132075267495e-40), ('it', 3.154986578408052e-37)]\n",
      "Sentence 4: [('es', 4.9174416374272643e-67), ('da', 1.2478524732084786e-82), ('ro', 2.339546205218211e-79), ('pt', 2.486213490992618e-55), ('it', 1.6930329611498644e-76)]\n",
      "Sentence 5: [('es', 1.1404743387052274e-165), ('da', 1.1135689980741882e-151), ('ro', 3.4776194476828845e-153), ('pt', 1.806738776043443e-188), ('it', 9.818595384522628e-152)]\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"Ey bro que no te coscas\".lower()\n",
    "print(\"Sentence 1:\", which_language(sent1, probability_distributions))\n",
    "sent2 = \"Brandon ganó 9 premios aprox.\".lower()\n",
    "print(\"Sentence 2:\", which_language(sent2, probability_distributions))\n",
    "sent3 = \"Pietro veut acheter une playstation\".lower()\n",
    "print(\"Sentence 3:\", which_language(sent3, probability_distributions))\n",
    "sent4 = \"illo pollica k m'enterao k la ma tadicho k nanai de juga a la play\".lower()\n",
    "print(\"Sentence 4:\", which_language(sent4, probability_distributions))\n",
    "sent5 = \"La dualidad onda-partícula en la mecánica cuántica desafía nuestra intuición clásica sobre la naturaleza fundamental de las partículas subatómicas.\".lower()\n",
    "print(\"Sentence 5:\", which_language(sent5, probability_distributions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you improve the models so that these errors do not happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Your ideas here:\n",
    "\n",
    "    - I think that training using broader corpora rather than just the bible would help a lot. \n",
    "    - Important to control loan words in the different languages. (train the model with loan words for each specific language with subcorpus specific examples)\n",
    "    - Having control over numbers and how they influence the model results could be also relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Sentiment analysis\n",
    "\n",
    "#### Naive Bayes in movie review data\n",
    " \n",
    " * Pang, B., Lee, L., & Vaithyanathan, S. (2002, July). [Thumbs up?: sentiment classification using machine learning techniques.](http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf) In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10 (pp. 79-86). Association for Computational Linguistics.\n",
    " \n",
    " * You can download the data from [this website](http://www.cs.cornell.edu/people/pabo/movie-review-data/) (There are different versions, let's all download the 1.1 version of the polarity dataset (`polarity dataset v1.1 (2.2Mb) (includes README.1.1):...`))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filetowordlist(path, sfx):\n",
    "    words = []\n",
    "    for item in sorted(os.listdir(path)):\n",
    "        if sfx in item:\n",
    "            f=open(path + item, encoding=\"iso8859-1\")\n",
    "            lines = [line.strip() for line in f]\n",
    "            f.close()\n",
    "            wordsinfile = []\n",
    "            for l in lines:\n",
    "                sentencewords = l.split()\n",
    "                wordsinfile = wordsinfile + sentencewords\n",
    "            words.append(wordsinfile)\n",
    "    return words\n",
    "\n",
    "def log(number):\n",
    "    return np.log(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: YOU MUST ADJUST THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the directory below\n",
    "#Put the directory where you downloaded the corpus\n",
    "\n",
    "posreviews_all = filetowordlist(\"/home/jesus/languageProcessing/Assignments/tokens/pos/\", \".txt\")\n",
    "negreviews_all = filetowordlist(\"/home/jesus/languageProcessing/Assignments/tokens/neg/\", \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set 550 out of the positive reviews and 550 out of the negative reviews for creating a model\n",
    "#We will teach the model how to make predictions using that portion of the data\n",
    "posreviews_train = posreviews_all[:550]\n",
    "negreviews_train = negreviews_all[:550]\n",
    "\n",
    "#And later, we will be able to use the remaining part to see how well our model performs\n",
    "#This is called the test data\n",
    "posreviews_test  = posreviews_all[550:]\n",
    "negreviews_test  = negreviews_all[550:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 550, 144, 142)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the number of positive and negative reviews in each of the portions of the data (train and test)\n",
    "len(posreviews_train), len(negreviews_train),len(posreviews_test), len(negreviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are two lists with all words in each group of reviews\n",
    "#poswords_train contains a list of words, with concatenated positive reviews\n",
    "poswords_train=[word for sent in posreviews_train for word in sent]\n",
    "negwords_train=[word for sent in negreviews_train for word in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at a couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'novels', '\"', \"carlito's\", 'way', '\"', 'and', '\"', 'after', 'hours', '\"', 'by', 'edwin', 'torres', ')', 'starring', ':', 'al', 'pacino', ',', 'sean', 'penn', ',', 'penelope', 'ann', 'miller', ',', 'john', 'leguiziamo', ',', 'luis', 'guzman', ',', 'john', 'rebhorn', ',', 'viggio', 'mortensen', ',', 'jorge', 'porcel', \"what's\", 'shocking', 'about', '\"', \"carlito's\", 'way', '\"', 'is', 'how']\n"
     ]
    }
   ],
   "source": [
    "print (poswords_train[1000:1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'being', 'semi-consoled', 'by', 'brooke', ',', 'one', 'of', 'his', 'graduate', 'students', '.', 'he', 'lives', 'on', 'arlington', 'road', 'where', 'he', 'makes', 'friends', 'with', 'neighbors', 'oliver', '(', 'robbins', ')', 'and', 'cheryl', 'lang', '(', 'cusack', ')', '.', 'then', 'he', 'begins', 'to', 'suspect', 'that', 'oliver', 'is', 'not', 'the', 'architect', 'he', 'claims', 'to', 'be', ',']\n"
     ]
    }
   ],
   "source": [
    "print (negwords_train[1000:1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabularies for positive and negative reviews\n",
    "pos_vocab_train = set(poswords_train)\n",
    "neg_vocab_train = set(negwords_train)\n",
    "vocab_train = pos_vocab_train.union(neg_vocab_train)\n",
    "\n",
    "#Number of types (vocabulary size)\n",
    "pos_vocab_size_train = len(pos_vocab_train)\n",
    "neg_vocab_size_train = len(neg_vocab_train)\n",
    "vocab_size_train = len(vocab_train)\n",
    "\n",
    "#Number of words (tokens)\n",
    "noposwords_train=len(poswords_train)\n",
    "nonegwords_train=len(negwords_train)\n",
    "\n",
    "#Number of reviews\n",
    "noposreviews_train=len(posreviews_train)\n",
    "nonegreviews_train=len(negreviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27813, 445289, 550)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vocab_size_train,noposwords_train,noposreviews_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6094379124341003, 0.6989700043360189)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(5),np.log10(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1 (12 pts):\n",
    "Calculate prior probabilities, which are the probabilities of $P(C=positive)$ and $P(C=negative)$.\n",
    "\n",
    "Instead of saving probabilities, remember the practical issue that we mentioned at the end of the class (we save logprobs (f.ex. `np.log`) instead of regular probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the log probability of a review to be positive:\n",
      "-0.6931471805599453\n",
      "This is the log probability of a review to be negative:\n",
      "-0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "prior_probabiolity_pos_train = log(noposreviews_train/(noposreviews_train+nonegreviews_train))\n",
    "prior_probabiolity_neg_train = log(nonegreviews_train/(noposreviews_train+nonegreviews_train))\n",
    "\n",
    "print (\"This is the log probability of a review to be positive:\")\n",
    "print (prior_probabiolity_pos_train)\n",
    "print (\"This is the log probability of a review to be negative:\")\n",
    "print (prior_probabiolity_neg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tricky part:\n",
    "\n",
    "Now we need to estimate the probability of a given word, given a specific class.\n",
    "\n",
    "$$P\\left(w_{k} | c_{i}\\right)=\\frac{\\operatorname{count}\\left(w_{k}, c_{i}\\right)+1}{\\sum_{w \\in V} \\operatorname{count}\\left(w, c_{i}\\right) + |V|}$$\n",
    "\n",
    "$$ logP\\left(w_{k} | c_{i}\\right) = log (P\\left(w_{k} | c_{i}\\right))$$\n",
    "\n",
    "We need some word counts, then.\n",
    "\n",
    "#### Exercise 3.2 (12 pts):\n",
    "Calculate word counts for the positive reviews and save them in a variable.\n",
    "\n",
    "Calculate also the word counts for the negative reviews and save them in another variable.\n",
    "\n",
    "##### Hint: You can use a dictionary (for each document class or polarity) to save the word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word nice appears 118 times in the positive reviews\n",
      "The word nice appears 92 times in the negative reviews\n",
      "\n",
      "The word bad appears 223 times in the positive reviews\n",
      "The word bad appears 509 times in the negative reviews\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "pos_frequencies = {}\n",
    "neg_frequencies = {}\n",
    "\n",
    "for review in posreviews_train:\n",
    "    for word in review:\n",
    "        if word in pos_frequencies:\n",
    "            pos_frequencies[word] += 1\n",
    "        else:\n",
    "            pos_frequencies[word] = 1\n",
    "for review in negreviews_train:\n",
    "    for word in review:\n",
    "        if word in neg_frequencies:\n",
    "            neg_frequencies[word] += 1\n",
    "        else:\n",
    "            neg_frequencies[word] = 1     \n",
    "    \n",
    "print (\"The word nice appears \"+str(pos_frequencies[\"nice\"])+\" times in the positive reviews\")\n",
    "print (\"The word nice appears \"+str(neg_frequencies[\"nice\"])+\" times in the negative reviews\")\n",
    "\n",
    "print ()\n",
    "\n",
    "print (\"The word bad appears \"+str(pos_frequencies[\"bad\"])+\" times in the positive reviews\")\n",
    "print (\"The word bad appears \"+str(neg_frequencies[\"bad\"])+\" times in the negative reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3 (12 pts):\n",
    "Now estimate the probability of each word to appear in a positive review. Do the same with negative reviews. When you save them, do as you did in the first exercise, and save log probabilities.\n",
    "\n",
    "##### Hint: You can use a dictionary (for each document class or polarity) to save the word logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log probability of the word nice to appear in the positive reviews is: -8.310737353530826\n",
      "The log probability of the word nice to appear in the negative reviews is: -8.438475770037291\n",
      "\n",
      "The log probability of the word bad to appear in the positive reviews is: -7.678214794787315\n",
      "The log probability of the word bad to appear in the negative reviews is: -6.736664537472175\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "pos_logprobs = {}\n",
    "neg_logprobs = {}\n",
    "for word in pos_frequencies:\n",
    "    pos_logprobs[word] = np.log((pos_frequencies[word] + 1) / (noposwords_train + vocab_size_train))\n",
    "for word in neg_frequencies:\n",
    "    neg_logprobs[word] = np.log((neg_frequencies[word] + 1) / (nonegwords_train + vocab_size_train))\n",
    "\n",
    "\n",
    "print (\"The log probability of the word nice to appear in the positive reviews is: \"+str(pos_logprobs[\"nice\"]))\n",
    "print (\"The log probability of the word nice to appear in the negative reviews is: \"+str(neg_logprobs[\"nice\"]))\n",
    "\n",
    "print ()\n",
    "\n",
    "print (\"The log probability of the word bad to appear in the positive reviews is: \"+str(pos_logprobs[\"bad\"]))\n",
    "print (\"The log probability of the word bad to appear in the negative reviews is: \"+str(neg_logprobs[\"bad\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.4 (12 pts):\n",
    "In order to have a clean implementation, estimate the log probability of a word that doesn't appear in the training set.\n",
    "\n",
    "This will make the code of the next exercise cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log probability of an out of vocabulary word in the positive reviews is:\n",
      "-13.089860846642354\n",
      "The log probability of an out of vocabulary word in the negative reviews is:\n",
      "-12.971075263190547\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "pos_oov_word_logprob = np.log(1 / (noposwords_train + vocab_size_train))\n",
    "neg_oov_word_logprob = np.log(1 / (nonegwords_train + vocab_size_train))\n",
    "\n",
    "print (\"The log probability of an out of vocabulary word in the positive reviews is:\")\n",
    "print (pos_oov_word_logprob)\n",
    "print (\"The log probability of an out of vocabulary word in the negative reviews is:\")\n",
    "print (neg_oov_word_logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.5 (12 pts):\n",
    "\n",
    "Now we just need to write a function that will return True if an input review is positive, and False if the input review is negative.\n",
    "\n",
    "$$\\operatorname{argmax}\\left(\\log P\\left(c_{i}\\right)+\\sum_{k=1}^{N} \\log P\\left(w_{k} | c_{i}\\right)\\right)$$\n",
    "\n",
    "How would you do that?\n",
    "\n",
    "##### Hint: Keep it simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_or_not(s):\n",
    "    #YOUR CODE HERE\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for word in s:\n",
    "        pos += np.log(noposwords_train / (vocab_size_train)) + np.log((pos_frequencies[word] + 1) / (noposwords_train + vocab_size_train))\n",
    "        neg += np.log(nonegwords_train / (vocab_size_train)) + np.log((neg_frequencies[word] + 1) / (nonegwords_train + vocab_size_train))\n",
    "    if pos > neg:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"the movie is nice\".split()\n",
    "positive_or_not(review_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"the movie is bad\".split()\n",
    "positive_or_not(review_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"such an awful movie\".split()\n",
    "positive_or_not(review_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_test = \"lovely experience\".split()\n",
    "positive_or_not(review_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
